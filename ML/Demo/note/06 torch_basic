# 导包
import torch

# 创建张量
x=torch.empty(5,3)

# 创建随机张量
x=torch.randn((5,3))

# 创建指定类型的张量
x = torch.zeros(5, 3, dtype=torch.long)
x = torch.ones(5, 3, dtype=torch.double)

# 从列表创建张量
x = torch.zeros(5, 3, dtype=torch.long)

# 重新定义数据类型
x = torch.randn_like(x, dtype=torch.float)

# 打印张量维度
print(x.size())

# 张量加法
y = torch.rand(5, 3)
print(x + y)
# 方法二
# print(torch.add(x, y))
# 方法三
# result = torch.empty(5, 3)
# torch.add(x, y, out=result)
# print(result)
# 方法四
# y.add_(x)
# print(y)

# 取张量的一列
print(x[:, 1])

# 张量变形
x = torch.randn(4, 4)
y = x.view(16)
print(x.size(),y.size())

y = x.view(2, 8)
print(x.size(),y.size())

# 方法二
z = x.view(-1, 8) # 确定一个维度，-1的维度会被自动计算
print(x.size(),z.size())

# 从张量中取出数字
x = torch.randn(1)
print(x)
print(x.item())


# 张量2numnpy
a = torch.ones(5)
print(a)

b = a.numpy()
print(b)

# 加法操作
a.add_(1)
print(a)
print(b)

# numpy2张量
import numpy as np
a = np.ones(5)
b = torch.from_numpy(a)
print(a)
print(b)

# 加法
np.add(a, 1, out=a)
print(a)
print(b)

# 自动微分
x = torch.ones(2, 2, requires_grad=True)
print(x)
y = x + 2
print(y)
print(y.grad_fn) # y就多了一个AddBackward
z = y * y * 3
out = z.mean()
print(z) # z多了MulBackward
print(out) # out多了MeanBackward

# 梯度
# 反向传播
out.backward()

# 打印梯度
print(x.grad) #out=0.25*Σ3(x+2)^2

x = torch.randn(3, requires_grad=True)
y = x * 2
while y.data.norm() < 1000:
    y = y * 2

print(y)
# 计算梯度
v = torch.tensor([0.1, 1.0, 0.0001], dtype=torch.float)
y.backward(v)

print(x.grad)


# 关闭梯度
print(x.requires_grad)
print((x ** 2).requires_grad)

with torch.no_grad():
    print((x ** 2).requires_grad)

# 方法二
# print(x.requires_grad)
# y = x.detach()
# print(y.requires_grad)
# print(x.eq(y).all())

# 神经网络
import torch
import torch.nn as nn
import torch.nn.functional as F


class Net(nn.Module):

    def __init__(self):
        super(Net, self).__init__()
        # 26.定义①的卷积层，输入为32x32的图像，卷积核大小5x5卷积核种类6
        self.conv1 = nn.Conv2d(3, 6, 5)
        # 27.定义③的卷积层，输入为前一层6个特征，卷积核大小5x5，卷积核种类16
        self.conv2 = nn.Conv2d(6, 16, 5)
        # 28.定义⑤的全链接层，输入为16*5*5，输出为120
        self.fc1 = nn.Linear(16 * 5 * 5, 120)  # 6*6 from image dimension
        # 29.定义⑥的全连接层，输入为120，输出为84
        self.fc2 = nn.Linear(120, 84)
        # 30.定义⑥的全连接层，输入为84，输出为10
        self.fc3 = nn.Linear(84, 10)

    def forward(self, x):
        # 31.完成input-S2，先卷积+relu，再2x2下采样
        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))
        # 32.完成S2-S4，先卷积+relu，再2x2下采样
        x = F.max_pool2d(F.relu(self.conv2(x)), 2) #卷积核方形时，可以只写一个维度
        # 33.将特征向量扁平成行向量
        x = x.view(-1, 16 * 5 * 5)
        # 34.使用fc1+relu
        x = F.relu(self.fc1(x))
        # 35.使用fc2+relu
        x = F.relu(self.fc2(x))
        # 36.使用fc3
        x = self.fc3(x)
        return x


net = Net()
print(net)

# 打印网络参数
params = list(net.parameters())
# print(params)
print(len(params))
# 打印参数形状
print(params[0].size())

# 查看前向传播
input = torch.randn(1, 1, 32, 32)
out = net(input)
print(out)

# 梯度初始化
net.zero_grad()

# 反向传播
out.backward(torch.randn(1, 10))

# 定义损失函数
criterion = nn.MSELoss()

# 计算损失
target = torch.randn(10)  # 随机真值
target = target.view(1, -1)  # 变成行向量

output = net(input)  # 用随机输入计算输出

loss = criterion(output, target)  # 计算损失
print(loss)

# 梯度初始化
net.zero_grad()

print('conv1.bias.grad before backward')
print(net.conv1.bias.grad)

# loss反向传播
loss.backward()

print('conv1.bias.grad after backward')
print(net.conv1.bias.grad)

# 更新权重
# 创建优化器
import torch.optim as optim
optimizer = optim.SGD(net.parameters(), lr=0.01)

# 使用优化器进行更新
optimizer.zero_grad()
output = net(input)
loss = criterion(output, target)
loss.backward()

# 更新权重
optimizer.step()





